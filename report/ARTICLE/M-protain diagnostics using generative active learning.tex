\documentclass[letterpaper]{article}
\usepackage{aaai20}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage[hyphens]{url}
\usepackage{graphicx}
\urlstyle{rm}
\def\UrlFont{\rm}
\usepackage{graphicx}
\frenchspacing
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
% Add additional packages here, but check
% the list of disallowed packages
% (including, but not limited to
% authblk, caption, CJK, float, fullpage, geometry,
% hyperref, layout, nameref, natbib, savetrees,
% setspace, titlesec, tocbibind, ulem)
% and illegal commands provided in the
% common formatting errors document
% included in the  Author Kit before doing so.
%
% PDFINFO
% You are required to complete the following
% for pass-through to the PDF.
% No LaTeX commands of any kind may be
% entered. The parentheses and spaces
% are an integral part of the
% pdfinfo script and must not be removed.
%
\pdfinfo{
/Title (M-Protain Diagnostics Using Generative Active Learning)
/Author (Hanyu Li, Junsong Yuan)
/Keywords (Active Learning, Generative Adversarial Nets, M-Protain)
}
%
% Section Numbers
% Uncomment if you want to use section numbers
% and change the 0 to a 1 or 2
% \setcounter{secnumdepth}{0}
% Title and Author Information Must Immediately Follow
% the pdfinfo within the preamble
%


\title{M-Protain Diagnostics Using Generative Active Learning}


\author{Hanyu Li\\
Department of Physics\\
Tsinghua University\\
l-hy16@mails.tsinghua.edu.cn
\And
Junsong Yuan\\
Department of Computer Science and Engineering\\
University at Buffalo\\
jsyuan@buffalo.edu}


\begin{document}
\maketitle


\begin{abstract}
    With comparatively less labeled data and high labeling cost, most of the medical involved tasks can not be directly tackled by state of art machine learning approaches for their lack of large carefully labeled datasets. Our paper is based on the a dataset of immunofixation electrophoresis(IFE) images used in the M-protain diagnostics that has no annotation. In order to make the diagnostics process more efficient, our paper try to train a binary classifier(normal or not) with only few data instance labeled by human experts and all other unlabeled data. We do the semi-supervised training by combining active learning with generative models. In our proposed method, we do these things iteratively: first we find the most uncertain data instances in the latent space of the generative model using the classifier; then we generate synthetic IFE images for human oracle to annotate; afterwards we add these labeled data back in the training set of the classifier. In addition, according to prior knowledge of the IFE images, we propose a specific explainable generative model based on Gaussian mixture model(GMM) that is only effective in this dataset, and compare the result of it with universal effective generative model like GAN and VAE. We conduct extensive experiments to demonstrate the difference between applied generative models, evaluate the effect they make on active learning quantitavely, and explore the reason behind the results.
\end{abstract}


\section{Introduction}
\par As deep models achieve astonishing results in almost every machine learning tasks, some unavoidable problems such as the need for large carefully labeled dataset has troubled researchers from the start. Part of the reason behind the tremendous success in deep learning is the availability of large-scale labeled data\cite{sun2017revisiting}. Although data labeling companies and platforms claim that they can provide inexpensive yet high quality data\cite{buhrmester2011amazon}, achieving such datasets can be extremely costly or even unrealistic in the scenarios where labeling requires high professionality. For instance, some medical image tasks can not be labeled by people without systematic training. However, the small number of these experts has determined that large-scale dataset is difficult to biuld. Plus, they are probably already preoccupied.

M-protain stands for Myeloma protein, which can be identified by applying immunofixation electrophoresis(IFE) because its sharp monoclonal band in the image. Different categories of results may indicate MGUS, smouldering myeloma(sMM), or multiple myeloma(MM). It usually takes three doctors to examine the IFE image and reach the final conclusion. Therefore the process is highly time consuming. In real world scenarios, more than half of the electrophoresis results are obviously normal which do not need further concern. Although final decision should be made by doctors, if a classifier can give an indicating result, then time can be greatly saved. Every IFE image consists of five columns of one dimensional signal. According to these columns, a 12 dimensional 0/1 vector will be computed as output. Part of the proposed method can be implemented by manually constructed rules, so in the end, machine learning involved section is narrowed down to a binary classification(normal or abnormal) of one dimensional signal.%这一段有一些要放到preliminary里面

A large dataset of IFE images is accessible, but each of them comes with a diagnostics report instead of the binary output of each column. Thus, due to the lack of explicit label, this is a classic semi-supervised situation. Our paper tries to utilize the combination of active learning and generative model to tackle with this unlabeled dataset. Active learning is that a machine learning algorithm that can achieve greater accuracy with fewer labeled training instances if it is allowed to choose the training data from an unlabeled dataset\cite{settles2009active}. When using same amount of labeled data, active learning tends to achieve better results. Thus, active learning techniques significantly reduce the amount of labor required compared to manually label all existing data. Deep generative models including GAN and VAE are currently purveiling in a variety of applications. Firstly proposed by Goodfellow \emph{et al.} in 2014, generative adversarial nets(GANs) has drawn a lot attention in the field of computer vision, natual language processing, and etc. Previous researchers inspired by its adversarial structure developed a large amount of variations that can be applied to a variety of tasks. Some of the most well-known models are WGAN\cite{arjovsky2017wasserstein}, CGAN\cite{mirza2014conditional}, BiGAN\cite{donahue2016adversarial}, VAEGAN\cite{larsen2015autoencoding}, and so on. Variation auto encoder(VAE) on the other hand, tries to develop a generative model in a variation inference kind of way\cite{kingma2013auto}. Although more robust to small pertubations in latent space, VAE tends to lose more graphic details when reconstructing the input data. Zhu and Bento made the first attempt\cite{Zhu2017GenerativeAA} to generate data for active learning process. Since then, a number of research have been conducted to find the most effective way to boost the active learning performance using generative models.%这一段有一些要放到related work里面

Our paper try to utilize the combination of active learning and generative models to gain a task learner for IFE column binary classification. By generating(decoding) synthetic IFE images based on latent space of the generative models, the model can query the least certain generated instances in the latent space, and thus improve the classifier in the latent space iteratively. In addition, according to prior knowledge of the IFE images, we propose a specific explainable generative model based on Gaussian mixture model(GMM) that is only effective in this dataset, and compare the result of it with universal effective generative model like GAN and VAE. We conduct extensive experiments to demonstrate the difference between applied generative models, evaluate the effect they make on active learning quantitavely, and try to find the reason behind it.%这一段根据abstract改


\section{Related Work}
Multiple techniques were used to tackle with small datasets and partially labeled datasets including active learning\cite{settles2009active}, generative models\cite{goodfellow2014generative}\cite{kingma2013auto}, data augmentation\cite{tanner1987calculation}, and domain transfer\cite{pan2009survey} etc. \cite{Zhu2017GenerativeAA}

\section{Preliminaries}
The problem is defined as below

\section{Proposed Method}
Our method make use of

\section{Experiments}
I did such experiments

\section{Discussion}
After I did these experiments

\section{Conclusion}
To sum up

% References and End of Paper
% These lines must be placed at the end of your paper
\bibliography{lib}
\bibliographystyle{aaai}
\end{document}