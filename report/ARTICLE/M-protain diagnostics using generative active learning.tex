\documentclass[letterpaper]{article}
\usepackage{aaai20}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage[hyphens]{url}
\usepackage{graphicx}
\urlstyle{rm}
\def\UrlFont{\rm}
\usepackage{graphicx}
\frenchspacing
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}
% Add additional packages here, but check
% the list of disallowed packages
% (including, but not limited to
% authblk, caption, CJK, float, fullpage, geometry,
% hyperref, layout, nameref, natbib, savetrees,
% setspace, titlesec, tocbibind, ulem)
% and illegal commands provided in the
% common formatting errors document
% included in the  Author Kit before doing so.
%
% PDFINFO
% You are required to complete the following
% for pass-through to the PDF.
% No LaTeX commands of any kind may be
% entered. The parentheses and spaces
% are an integral part of the
% pdfinfo script and must not be removed.
%
\pdfinfo{
/Title (M-Protain Diagnostics Using Generative Active Learning)
/Author (Hanyu Li, Junsong Yuan)
/Keywords (Active Learning, Generative Adversarial Nets, M-Protain)
}
%
% Section Numbers
% Uncomment if you want to use section numbers
% and change the 0 to a 1 or 2
% \setcounter{secnumdepth}{0}
% Title and Author Information Must Immediately Follow
% the pdfinfo within the preamble
%


\title{M-Protain Diagnostics Using Generative Active Learning}

\iffalse
\author{Hanyu Li\\
Department of Physics\\
Tsinghua University\\
l-hy16@mails.tsinghua.edu.cn
\And
Junsong Yuan\\
Department of Computer Science and Engineering\\
University at Buffalo\\
jsyuan@buffalo.edu}
\fi

\author{}

\begin{document}
\maketitle


\begin{abstract}
    With comparatively less labeled data and high labeling cost, most of the medical involved tasks can not be directly tackled by state of art machine learning approaches for their lack of large carefully labeled datasets. Our paper is based on the a dataset of immunofixation electrophoresis(IFE) images used in the M-protain diagnostics that has no annotation. In order to make the diagnostics process more efficient, our paper try to train a binary classifier(normal or not) with only few data instance labeled by human experts and all other unlabeled data. We do the semi-supervised training by combining active learning with generative models. In our proposed method, we do these things iteratively: first we find the most uncertain data instances in the latent space of the generative model using the classifier; then we generate synthetic IFE images for human oracle to annotate; afterwards we add these labeled data back in the training set of the classifier. In addition, according to prior knowledge of the IFE images, we propose a specific explainable generative model based on Gaussian mixture model(GMM) that is only effective in this dataset, and compare the result of it with universal effective generative model like GAN and VAE. We conduct extensive experiments to demonstrate the difference between applied generative models, evaluate the effect they make on active learning quantitavely, and explore the reason behind the results.
\end{abstract}


\section{Introduction}
\par As deep models achieve astonishing results in almost every machine learning tasks, some unavoidable problems such as the need for large carefully labeled dataset has troubled researchers from the start. Part of the reason behind the tremendous success in deep learning is the availability of large-scale labeled data\cite{sun2017revisiting}. Although data labeling companies and platforms claim that they can provide inexpensive yet high quality data\cite{buhrmester2011amazon}, achieving such datasets can be extremely costly or even unrealistic in the scenarios where labeling requires high professionality. For instance, some medical image tasks can not be labeled by people without systematic training. However, the small number of these experts has determined that large-scale dataset is difficult to biuld. Plus, they are probably already preoccupied. With the desire to fill this gap, our paper combines active learning and generative model to achieve relatively good results on small datasets.

For instance, a large dataset of medical images regarding M-protain diagnostics is accessible. Nonetheless, each image comes with a diagnostics report without the trainable label that we desire. M-protain stands for Myeloma protein, which can be identified by applying immunofixation electrophoresis(IFE) because its sharp monoclonal band in the image. Different categories of results may indicate MGUS, smouldering myeloma(sMM), or multiple myeloma(MM). It usually takes three doctors to examine the IFE image and reach the final conclusion. Therefore the process is highly time consuming. In real world scenarios, more than half of the electrophoresis results are obviously normal which do not need further concern. Although final decision should be made by doctors, if a classifier can provide an indicating result, then time can be significantly saved. Part of the proposed method can be implemented by manually constructed rules, so in the end, machine learning involved section is narrowed down to a binary classification(normal or abnormal) of one dimensional signal.

Due to the lack of explicit label, this is a classic semi-supervised learning task. Our paper tries to utilize the combination of active learning and generative model to tackle with this unlabeled dataset. Active learning is that a machine learning algorithm that can achieve greater accuracy with same amount labeled training instances if it is allowed to choose the data to be labeled from an unlabeled dataset \cite{settles2009active}. Thus, active learning techniques significantly reduce the amount of labor required compared to manually label all existing data. Deep generative models including GAN and VAE are currently purveiling in a variety of applications. Zhu and Bento made the first attempt\cite{Zhu2017GenerativeAA} to generate data for active learning process. Since then, a number of research have been conducted to find the most effective way to boost the active learning performance using generative models.

Our paper also tries to utilize the combination of active learning and generative models to gain a task learner, but we made specific augmentations specifically for IFE column binary classification. By generating(decoding) synthetic IFE images based on latent space of the generative models, the model can query the least certain generated instances in the latent space, and thus improve the classifier in the latent space iteratively. In addition, according to prior knowledge of the IFE images, we propose a specific explainable generative model based on Gaussian mixture model(GMM) that is only effective in this dataset, and compare the result of it with universal effective generative model like GAN and VAE. We conduct extensive experiments to demonstrate the difference between applied generative models, evaluate the effect they make on active learning quantitavely, and try to find the reason behind it.


\section{Related Work}
Several techniques were widely used by researchers to tackle with small datasets and partially labeled datasets. For example, active learning\cite{settles2009active} tries to pick the most informative data to label so that models can learn better when the total number of labeled data is limited; generative methods\cite{kingma2014semi}\cite{springenberg2015unsupervised} wish to benifit target classifier using knowledge of data distribution learnt by generative models; data augmentation\cite{tanner1987calculation} enriches small dataset in several ways; domain transfer\cite{pan2009survey} utilizes large, easily acquired datasets in different tasks or different settings to help the training of target learner. In this paper we mainly focus on active learning and generative methods.
\subsection{Generative Methods}
Firstly proposed by Goodfellow \emph{et al.} in 2014, generative adversarial nets(GANs) \cite{goodfellow2014generative} drawn a lot attention in the field of computer vision, natual language processing, and etc. Previous researchers inspired by its adversarial structure developed a large amount of variations that can be applied to a large number of tasks. Variation auto encoder(VAE) on the other hand, tries to develop a generative model in a variation inference kind of way\cite{kingma2013auto}. Although more robust to small pertubations in latent space, VAE tends to lose more graphic details when reconstructing the input data. Some of the most well-known models are WGAN\cite{arjovsky2017wasserstein}, CGAN\cite{mirza2014conditional}, BiGAN\cite{donahue2016adversarial}, VAEGAN\cite{larsen2015autoencoding}, and so on. 
\subsection{Active Learning}
\subsection{Generative Active Learning}

\section{Preliminaries}
The entire defination of the task can be defined as below. Every IFE image consists of five columns of one dimensional signal. According to an IFE image, a 12 dimensional 0/1 vector will be computed as output.

\section{Proposed Method}
Our method make use of

\section{Experiments}
I did such experiments

\section{Discussion}
After I did these experiments

\section{Conclusion}
To sum up

% References and End of Paper
% These lines must be placed at the end of your paper
\bibliography{lib}
\bibliographystyle{aaai}
\end{document}